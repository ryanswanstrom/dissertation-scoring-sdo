\documentclass[SDSUThesis.tex]{subfiles} 
\begin{document}

\section{CUMULATIVE RESULT INDICATOR (CRI)}
\label{sec:CRI}

    Software development organizations struggle to measure overall performance.  
    The \textit{Cumulative Result Indicator (CRI)} is an algorithm to provide a single 
    number score to measure the performance of a software development organization. 
    It works by statistically analyzing the past performance of the 
    organization and using that information to score an organization 
    on current performance.  CRI is a collection of the following 5 
    elements, which are actually KRIs,
    for a software development organization.
    \textbf{
    \begin{enumerate}
        \item Quality
        \item Availability
        \item Satisfaction
        \item Schedule
        \item Requirements
    \end{enumerate}
    }
    A separate CRI
    score is calculated for each element and 
    then aggregated together to form an overall 
    CRI score.  A new CRI score will be calculated based
    upon the selection of a given time period (weekly, monthly, quarterly).
    \textbf{CRI is not meant to be comparative between
    organizations, but to measure the amount of increase or decrease
    a single organization exhibits across elements}.  CRI is made to be easily 
    expandable to other elements if desired.
    
    The scores for CRI will range from -1, indicating the worst 
    performance, all the way to +1, indicating perfection.
    A score of 0 is an indication of meeting the basic expectations. 
    A negative score indicates under-performance and a 
    positive score indicates over-performance.   Here are some 
    examples. An CRI score of 0.35 means the organization is 
    performing 35\% better than expected.  Conversely, a score 
    of -0.15 means an organization is performing 15\% worse than expected.  

    Below are the attributes of the CRI scoring.
    \begin{itemize}
        \item The range of scores must have equal values above and below 0
        \item The minimum score must equate to the worst possible 
            performance, however that is defined
        \item Similarily, the maximum score must equate to the best possible performance.  
        \item A score of 0 must be average (or expected) performance
        \item All individual elements must have the same scoring range
    \end{itemize}
            
    As long as those 5 features are met, the range of scores can be anything.  
    The range of $[-1,1]$ was chosen because it is easy to scale 
    to a different range such as $[-10,10]$ or $[-100,100]$.  
    Thus scaling can be applied to obtain values in any appropriate range. 
    The scaling factor is denoted with the variable $k$.  
    The scale must be the same
    for all 5 elements.
    
    \subsection{ELEMENTS OF CRI}
        Each of the 5 elements of CRI has its own set of data that needs to
        be collected and a formula for calculating a score. 
        These five elements will be outline in the next sections.
        
        \subsubsection{QUALITY}
            Measuring quality is a crucial part of accessing 
            software development results.  
            Poor quality means time, money, and resources are spent 
            fixing the problems. As a result, new features are not being
            created. One of the key indicators of software quality 
            is defects.  It is important
            to measure the number of defects associated with a software 
            release because industry-wide the current defect removal rate is only 
            about 85\% and this value should be increased to 
            about 95\% for high quality software \cite{Jones2009}. 
            Organizations are leaving too many defects unfixed.  If organizations
            could lower the number of defects, then not as many defects would
            need to be fixed, which in turn would raise the defect removal rate.
            
            Another aspect of defects is severity levels.  A severity level indicates
            the importance of a defect that has been discovered.  Although an
            organization can choose whatever severity levels they choose, it is
            common practice to use 5 severity levels \cite{Raynus1999}.  The most 
            severe level for a defect is 1.  All other levels drop in severity 
            from that point. Table \ref{tab:severity} describes the 5 levels
            for defect severity.  

            \begin{longtable}{@{}l l}
                \toprule%
                 \centering%
                 {\bfseries Level}
                 & {\bfseries Description}\\
                
                \cmidrule[0.4pt](r{0.125em}){1-1}%
                \cmidrule[0.4pt](lr{0.125em}){2-2}%
                % \midrule
                \endhead
                
                1 & Software is unavailable and no workaround   \\
                \myrowcolour%
                2 & Software performance degraded with no workaround \\
                3 & Software performance degraded but workaround exists \\
                \myrowcolour%
                4 & Software functions but a loss of non-critical functionality \\
                5 & Others: minor cosmetic issue, missing documentation \\
                
                \bottomrule
                
                \caption{SOFTWARE DEFECT SEVERITY LEVELS}
                \label{tab:severity}
            \end{longtable}
            
            \paragraph{QUALITY DATA}
                In order to properly score the quality of an SDO,
                certain data needs to be obtained in order to measure
                performance. Table \ref{tab:qualitydata} identifies
                the columns of data that will be used to create
                a score for the quality element of CRI.  
                Each column is classified as \textit{required} or
                \textit{optional}.  That is to allow some flexibility
                in the model for organizations that collect
                varying amounts of data.
                
                \begin{longtable}{@{}l rr rr}
                    % pairs: absolute number (percentage)
                    
                    \toprule%
                     \centering%
                     {\bfseries Column Name}
                     & {\bfseries Data Type}
                     &  \\
                    
                    \cmidrule[0.4pt](r{0.125em}){1-1}%
                    \cmidrule[0.4pt](lr{0.125em}){2-2}%
                    \cmidrule[0.4pt](l{0.125em}){3-3}%
                    % \midrule
                    \endhead
                    
                    Application ID & String (factor) & Required \\
                    \myrowcolour%
                    Frequency Date & Date & Required \\
                    Development Effort & Integer & Required \\
                    \myrowcolour%
                    Testing Effort & Integer & Optional \\ 
                    SIT Defects & Integer & Optional \\ 
                    \myrowcolour%
                    UAT Defects & Integer  & Optional \\ 
                    PROD Defects & Integer  & Required \\
                    
                    \bottomrule
                    
                    \caption{QUALITY DATA NEEDED FOR CRI}
                    \label{tab:qualitydata}
                \end{longtable}
                
                The development and testing effort can come from any of 
                the following choices for effort. It is possible
                that other measures will work for effort.
                \begin{description}
                    \item[Actual Time] This number is a representation of the 
                        total amount of time spent on a project.  This number
                        can be measured in any unit of time: hours, days, weeks,
                        etc.  Actual time can be applied to development or testing
                        effort.
                    \item[Estimated Time] This number is a representation of the 
                        initial estimated amount of time spent on a project.  This number
                        can be measured in any unit of time: hours, days, weeks,
                        etc.  Estimated time can be applied to development or testing
                        effort.  It is common for the estimated and actual times
                        to be diffent.
                    \item[Source Lines Of Code (SLOC)]  This number is the count of 
                        the total number of lines of source code for a project. 
                        Obviously, 
                        this item only counts as a level of
                        effort for development unless coding in used to generate
                        automated test cases. \footnote{Automated testing is the
                        process of creating software to automatically run tests
                        against other software.  The adoption of automated testing
                        is varied and it is not a solution in all cases 
                        \cite{Ramler2006}. }
                    \item[Modified Lines Of Code] This number is a count of the number 
                        of modified lines of source code. Modified lines is 
                        defined as the number of deleted, added, and modified 
                        lines of source code.  This number is different from above
                        since it does not include all the lines of source code.  Similar
                        to above, this number makes more sense for development effort.
                    \item[Test Cases] A test case is a step or series of steps followed
                        to validate some expected outcome of software.  Organizations
                        will create a number of test cases to be validated for a
                        software system.  The number of such test cases could
                        be used as a level of testing effort.
                \end{description}
                
                Notice the data does not include a severity level.  The severity
                level should be handled before being stored.  A good technique is
                to count the defects based upon the weighting scheme
                in Table \ref{tab:severityweight} \cite{Raynus1999}. For
                example, finding 1 defect of severity level 5 will result
                in total count of 1.  However, finding 1 defect of severity
                level 2 will result in a total count of 15. This strategy helps
                to standardize the number of defects found.  An organization
                can alter the values of Table \ref{tab:severityweight} based 
                upon priorities or use a different technique if desired.  It
                is just important to get a standard, meaningful number for SIT defects,
                UAT defects, and PROD defects which manages severity appropriately.
                
                \begin{longtable}{@{}l rr rr}
                    % pairs: absolute number (percentage)
                    
                    \toprule%
                     \centering%
                     {\bfseries Severity Level}
                     & {\bfseries Weight} \\
                    
                    \cmidrule[0.4pt](r{0.125em}){1-1}%
                    \cmidrule[0.4pt](lr{0.125em}){2-2}%
                    % \midrule
                    \endhead
                    
                    1 & 30 \\
                    \myrowcolour%
                    2 &  15 \\
                    3 &  5 \\
                    \myrowcolour%
                    4 &  2 \\ 
                    5 &  1 \\ 
                    
                    \bottomrule
                    
                    \caption{DEFECT SEVERITY LEVEL WEIGHTING}
                    \label{tab:severityweight}
                \end{longtable}
            
            \paragraph{QUALITY FORMULA}
                The first step in creating a score for the quality
                element is analysis of the historical data to create
                a baseline function.  The historical
                data is all quality data collected before  a given point
                in time.  Some common historical cutoffs are the current date
                or the end of the previous fiscal year.  Then a mathematical
                model, called the \textit{baseline quality function},
                to predict PROD Defects will be produced.  In statistical
                terms, the response is \textit{PROD Defects} and the predictors
                are: \textit{UAT Defects}, \textit{SIT Defects}, 
                \textit{Testing Effort}, and \textit{Development Effort}.  
                Some of the following strategies will be employed to find 
                a reasonable model.
                \begin{itemize}
                    \item Removal of outliers and/or influential points
                    \item Linear Regression
                    \item Stepwise Regression
                    \item Ridge Regression for suspected multicolinearity
                \end{itemize}
                
                Once a model has been found, it will be labeled as $f$ and it
                will not change.  The function $f$ can be the same for all 
                Application IDs or it can be different for each Application ID
                or any combination of Application IDs.   It serves as the 
                quality baseline for CRI. 
                All future quality scores will be dependent upon the original
                $f$.  Once set, the model will not change.
                
                After the model $f$ has been determined, it is time
                to calculate the quality score for each application ID
                within the given time period.  The quality score
                for each Application ID can be calculated as follows.
                
                \begin{displaymath}
                   S_{1_i} = \left\{
                     \begin{array}{lr}
                       \text{where } f_i \geq d_i & :  \frac{f_i - d_i}{f_i} \times k  \\
                       \text{where } d_i > f_i  & : \frac{f_i-d_i }{6\sigma_i} \times k
                     \end{array}
                   \right] \text{   , calculate quality score for each app $i$}
                \end{displaymath} 
                where
                \begin{itemize}
                    \item $S_{1_i}$ is the quality score for Application ID $i$
                    \item $n$ is the number of Application IDs
                    \item $d_i$ is the actual PROD defects
                    \item $f_i$ is the function to predict PROD defects for Application $i$ based upon 
                        \textit{UAT Defects}, \textit{SIT Defects}, \textit{Testing Effort}, 
                        and \textit{Development Effort}
                    \item $\sigma^2$ is the estimated variance
                \end{itemize}
                
                Then the overall quality score is calculated as below.
                
                \[
                    S_{1} = \sum\limits^n_{i=1} w'_i S_{1_i} \text{ where } \sum\limits^n_{i=1} w'_i = 1
                \]
                where
                \begin{itemize}
                    \item $S_1$ is the combined quality score for all Application IDs, 
                    a weighted average
                \end{itemize}
                
                Then $S_1$ represents the quality score for that given time period.
        


        \subsubsection{AVAILABILITY}
            All the new requirements and great quality do not matter if the software is not available.
            All the new features and great quality do not matter if the software is not available.  Thus it
            is essential to set an expected Service Level Agreement (SLA)\footnote{For an SDO, the SLA is a 
            contract specifying the amount of time software will be available during a
            given time period. } and measure 
            performance against that SLA.  The following section will outline to data needed to properly
            calculate an SLA and the data needed to calculate the CRI score for availability.
            
            Special Note: The Service ID for availability does not have to be the same
            as the Application ID for quality or any of the other elements.  Some 
            organizations have a one-to-one mapping between Applications being developed
            and services being deployed.  Others have more complex sceranios that require
            multiple applications to be combined to form a service.  Then the availability
            of the system is tracked.
            
            \paragraph{AVAILABILITY DATA}
                Table \ref{tab:availdata} identifies the necessary data to calculate
                the CRI element score for availability.  Notice the three optional 
                fields: \textit{Uptime}, \textit{Scheduled Downtime}, and 
                \textit{Unscheduled Downtime}; they are optional because they can be
                used to calculated the \textit{Percent Uptime}.  The \textit{Percent Uptime}
                is the important value for the CRI schedule score.  Here are the two common
                approaches for calculating percent uptime:
                
                The preferred method:
                \[
                    \text{Percent Uptime} = \frac{\text{Uptime}}{\text{Uptime + Scheduled Downtime + Unscheduled Downtime}}
                \]
                and the alternative method:
                \[
                    \text{Percent Uptime} = \frac{\text{Uptime}}{\text{Uptime + Unscheduled Downtime}}
                \]
                The only difference is the removal of scheduled downtime from 
                the calculation.  The calculation approach is typically specified in
                the contract associated with the SLA.  Thus, the Percent Uptime is important
                and it can either be supplied in the data or calculated from the 
                optional fields.
            
            
                \begin{longtable}{@{}l rr rr}
                    \toprule%
                     \centering%
                     {\bfseries Column Name}
                     & {\bfseries Data Type}
                     &  \\
                    
                    \cmidrule[0.4pt](r{0.125em}){1-1}%
                    \cmidrule[0.4pt](lr{0.125em}){2-2}%
                    \cmidrule[0.4pt](l{0.125em}){3-3}%
                    % \midrule
                    \endhead
                    
                    Service ID & String  & Required \\
                    \myrowcolour%
                    Frequency Date & Date & Required \\
                    Uptime & Float & Optional \\
                    \myrowcolour%
                    Scheduled Downtime & Float & Optional \\
                    Unscheduled Downtime & Float  & Optional \\
                    \myrowcolour%
                    Percent Uptime & Float & Required \\
                    Expected Percent Uptime & Float & Required \\
                    
                    \bottomrule
                    
                    \caption{AVAILABILITY DATA NEEDED FOR CRI}
                    \label{tab:availdata}
                \end{longtable}
            
            \paragraph{AVAILABILITY FORMULA}
                The formula for availability is more straightforward than the quality formula.
                It does not include any analysis of the historic data.  That lack of historical
                analysis is avoided since the SLA provides an existing baseline to measure 
                against.  The following formula is simply a percentage the SLA was exceeded
                or missed. 
                
                \begin{displaymath}
                   S_{2_i} = \left\{
                     \begin{array}{lr}
                       \text{where } A_{a_i} \leq A_{e_i} & : \left[ \frac{A_{a_i} - A_{e_i}}{A_{e_i}}\times k \right] \\
                       \text{where } A_{a_i} > A_{e_i}  & : \left[ \frac{A_{a_i} - A_{e_i} }{1-A_{e_i}}\times k \right]
                     \end{array}
                   \right. \text{   , calculate availability score for each app $i$}
                \end{displaymath}
                
                where
                
                \begin{itemize}
                    \item $A_{a_i}$ actual availability for System ID $i$
                    \item $A_{e_i}$ expected availability for System ID $i$
                \end{itemize}
                
                Then the overall availability score is calculated as below.
                
                \[
                    S_{2} = \sum\limits^n_{i=1} w'_i S_{2_i} \text{ where } \sum\limits^n_{i=1} w'_i = 1
                \]
                
        \subsubsection{SATISFACTION}
            The satisfaction of users, customers, and/or business partners is 
            the third element to be measured.  This element is important because 
            in an established business, retaining customers is less expensive
            than attracting new customers \cite{Aulet2013}.  
            Depending upon the type of SDO,
            the customers may be internal or external to the organization.  For
            the remainder of this section, the term customer will be used
            to represent any person who is responsible for guidance, decision-making
            or use of the software.  The term customer can refer to a: user, paying or nonpaying
            customer, internal or external business partner, or any other person deemed influential
            to the development of the software.
            
            \textit{If one element of CRI was to be rated as the most important, satisfaction
            would be it.}  
            Without satisfied customers, the rest of the measures do not matter.
            For example, having a quality
            application that is always available does not matter if the
            application is not what the customer wants.  
            
            Surveys are used to measure satisfaction for CRI.  A series of statements will
            be presented to all or a subset of the customers.  Any customer that chooses
            to respond to the survey is considered a respondent.  A respondent can rate
            statements based upon a Likert Scale\footnote{"The Likert Scale presents 
            respondents with a series of (attitude) dimensions, which fall along a
            continuum." \cite{Cowles2015}} with a numerical response where the 
            minimum value indicates maximum
            disagreement and the maximum value indicates
            the maximum agreement. Common rating scales would be
            from 1 to 5 or from 1 to 3.  An example survey can be seen in Table 
            \ref{tab:samplesurvey}.
            
            
            \begin{longtable}{@{}l rr rr}
                \toprule%
                 \centering%
                 {\bfseries ID}
                 & {\bfseries Statement}
                 & {\bfseries Disagree}
                 & {\bfseries Neutral}
                 & {\bfseries Agree}  \\
                
                \cmidrule[0.4pt](r{0.125em}){1-1}%
                \cmidrule[0.4pt](lr{0.125em}){2-2}%
                \cmidrule[0.4pt](l{0.125em}){3-3}%
                \cmidrule[0.4pt](l{0.125em}){4-4}%
                \cmidrule[0.4pt](l{0.125em}){5-5}%
                % \midrule
                \endhead
                
                1 & I find the software easy to use.  & & & \\
                \myrowcolour%
                2 & I would recommend this software to others. & & & \\
                3 & The software makes me more productive. & & & \\
                \myrowcolour%
                4 & I am happy with this software. & & & \\
                
                \bottomrule
                
                \caption{SAMPLE SURVEY FOR SATISFACTION}
                \label{tab:samplesurvey}
            \end{longtable}
            
            \paragraph{ISSUES WITH SURVEYS}
                Surveys present a number of challenges that need to be presented and briefly
                discussed.  Here are some of the issues that need to be addressed when
                using surveys. 
                
                \begin{description}
                \item[Text] \hfill \\
                    The specific text used in the questions or statments
                    is very important.  The text cannot be too vague.  Also, the text
                    must be clear enough to eliminate misinterpretation. Survey
                    questions must be complete and not include gaps.  For example, if
                    an age range is presented, it must include all possible ages.  These
                    are just some of the difficulties with getting the text correct
                    in surveys. 
                \item[Number and Ordering] \hfill \\
                    The number of questions is important.  Too many questions and the 
                    respondents will lose interest and begin responding without the
                    adequate attention needed.  Plus, if the survey is too long there
                    is the risk of quitting before completion.  A short survey might
                    not cover the adequate amount of material.  Both short and long
                    surveys run the risk of providing inaccurate responses.  After
                    determining the best number of questions, the ordering of the questions
                    is important.  Previous survey questions can have an unintended 
                    impact on responses.  Thus, the ordering of questions needs to be
                    addressed. 
                \item[Sampling] \hfill \\
                    Next is the issue of sampling.  Not every customer can be surveyed, so 
                    sample sets os customers need to be presented with a survery.  
                    In the case of CRI, there are 2 possible scenarios for sampling.  
                    \begin{description}
                    \item[First]
                    When an SDO is part of a larger organization, there typically is
                    a small number of business partners that help to guide and 
                    direct the work performed by the SDO.  In this case, the business 
                    partners might be the the ones offering survey responses and they 
                    should all be willing to respond.  Thus, those business partners 
                    represent the entire population, and the surveys should result in 
                    a 100\% response rate which is technically a census.  
                    The only bias that will be present here is the bias of the business 
                    partners and sampling cannot control for that.
           
                    \item[Second] End-users will be surveyed for satisfaction. Obviously, the 
                    entire population cannot be surveyed, so a probability sample should 
                    be randomly created. Even then, bias will be present.  
                        \begin{itemize}
                            \item Not all users will respond.  This is because survey 
                                respondents tend to sit at the extremes of either 
                                satisfied or dissatisfied.  Thus the results will tend 
                                to indicate that separation. 
                            \item  Even with probability sampling it is possible to miss 
                                entire groups of population members. For example consider a 
                                banking application such as a savings account, a survey would 
                                most likely be presented online and it would have a coverage 
                                bias due the exclusion of savings account holders that 
                                do not bank online.
                            \item A selection bias can occur when some members of the 
                                population have a higher probability of inclusion in 
                                the sampling frame than others.  One example could be 
                                a user with multiple savings accounts.  The selection 
                                bias is typically easy to avoid if the bias is 
                                identified.  Weighting is a common solution for selection bias. 
                        \end{itemize}
                       
                    \end{description}
                
                \end{description}
                
                
           
            For more on creating appropriate surveys, see \cite{Snijkers2013} by Snijkers,
            Haraldsen, Jones, and Willimack. In the book, Snijkers et al. present a framework
            named generic statistical business process model (GSBPM) for conducting surveys
            in a business or organizational setting.  GSBPM covers the issues above as well
            as a few more issues such as response storage and risks.  Also, Cowles and Nelson
            provide another good resource for preparing and conducting surveys 
            \cite{Cowles2015}.  They even include an entire chapters on both writing 
            survey questions and survey errors. 
            
            \paragraph{SATISFACTION DATA}
                Once the surveys have been distributed and the results collected, Table
                \ref{tab:satisfactiondata} displays the data that needs to be collected 
                in order to calculate the satisfaction element score for CRI.
                \begin{longtable}{@{}l rr rr}
                    \toprule%
                     \centering%
                     {\bfseries Column Name}
                     & {\bfseries Data Type}
                     &  \\
                    
                    \cmidrule[0.4pt](r{0.125em}){1-1}%
                    \cmidrule[0.4pt](lr{0.125em}){2-2}%
                    \cmidrule[0.4pt](l{0.125em}){3-3}%
                    % \midrule
                    \endhead
                    
                    Question ID & String  & Required \\
                    \myrowcolour%
                    Question Text & String  & Optional \\
                    Respondent ID & String & Optional \\
                    \myrowcolour%
                    Frequency Date & Date & Required \\
                    Response & Integer & Required \\
                    \myrowcolour%
                    Response Date & Date & Optional \\
                    Application ID & String & Optional \\
                    
                    \bottomrule
                    
                    \caption{SATISFACTION DATA NEEDED FOR CRI}
                    \label{tab:satisfactiondata}
                \end{longtable}
            
            \paragraph{SATISFACTION FORMULA}
                After collecting the necessary survey data from Table \ref{tab:satisfactiondata},
                calculating the score is rather straight forward.
                The scores for each question are averaged and then those
                values are averaged together.  If some survey questions
                are more important than others, the formula could be
                easily modified to include weighting.  
            
                First the score for each question needs to be calculated.
                
                \[
                    S_{3_i} = \sum^n_{i=1}\left( k \times \frac{ \sum^m_{j=1}a_{ij}-\frac{min + max}{2}}{m} \right)
                \]
                
                \begin{itemize}
                    \item $a_{ij}$ answer to question $i$ for respondent $j$
                    \item $n$ number of questions
                    \item $m$ number of respondents
                    \item $min$ minimum score for a question
                    \item $max$ maximum score for a question
                \end{itemize}
                
                Then the satisfaction score is calculated as below.  Use a weighted average
                to combine the question scores.
                
                \[
                    S_{3} = \sum\limits^n_{i=1} w'_i S_{3_i} \text{ where } \sum\limits^n_{i=1} w'_i = 1
                \]

        \subsubsection{SCHEDULE}  
            Delivery of software in a timely manner is an essential part of being a 
            successful SDO.  Being able to meet scheduled deadlines is a sign of accurate
            estimation and planning.  Drastically missing deadlines is a sign of an SDO
            with a process that needs refinement.  Studies have shown that 
            software projects exceed the estimates by an average of
            30\% \cite{Jorgensen2014}.  Thus it is important to score SDOs on accurate
            schedule adherence. Without tracking and measuring schedule adherence, it will
            not improve.  
            
            The CRI schedule score provides a numeric value to indicate the amount 
            schedules are missed or exceeded.  The score provides a cumulative measure
            of the performance as compared to other months.
            The score is based upon the historical deviance of estimates for
            projects.  Projects completing on time will be given a score of 0.
            Projects finishing early will be rewarded with positive scores increasing toward
            $k$. Alternatively, 
            late projects will be given negative scores that approach $-k$ as the projects
            become more late. 
            
            \paragraph{SCHEDULE DATA}
            
                In order to calculate the schedule score, certain dates
                need to be present. Table \ref{tab:scheduledata} outlines
                the necessary data for schedules.  One date is considered
                optional as it is not used in the CRI calculation, but it
                is an important date that could be useful for future
                enhancements to CRI.
            
                \begin{longtable}{@{}l rr rr}
                    \toprule%
                     \centering%
                     {\bfseries Column Name}
                     & {\bfseries Data Type}
                     &  \\
                    
                    \cmidrule[0.4pt](r{0.125em}){1-1}%
                    \cmidrule[0.4pt](lr{0.125em}){2-2}%
                    \cmidrule[0.4pt](l{0.125em}){3-3}%
                    % \midrule
                    \endhead
                    
                    Project ID & String  & Required \\
                    \myrowcolour%
                    Frequency Date & Date & Required \\
                    Scheduled Start Date & Date & Required \\
                    \myrowcolour%
                    Scheduled Finish Date & Date & Required \\
                    Actual Start Date & Date  & Optional \\
                    \myrowcolour%
                    Actual Finish Date & Date  & Required \\
                    
                    \bottomrule
                    
                    \caption{SCHEDULE DATA NEEDED FOR CRI}
                    \label{tab:scheduledata}
                \end{longtable}
                    
            \paragraph{SCHEDULE FORMULA}
                Schedule has a clear date for finishing on-time, however
                there are not clear bounds as to how early or late a project
                can be delivered.  Thus, the formula for schedule is more involved
                than availability or satisfaction.  It requires some analysis
                of the historical data.  The first step of the formula is
                determining how often projects are early or late, and by how
                much a project is early or late.  This can be accomplished
                by looking at the distribution of the data.  Specifically,
                look at what percentage of the entire project duration the
                schedule was missed.  
                
                \begin{displaymath}
                    \Delta_i = \frac{F_{a_i} - F_{s_i}}{ F_{s_i} - B_{s_i} + 1}
                \end{displaymath}
                where
                \begin{itemize}
                    \item $F_{a_i}$ the actual finish date of project $i$
                    \item $F_{s_i}$ the scheduled finish date of project $i$
                    \item $B_{s_i}$ the scheduled beginning date of project $i$
                    \item $\Delta_i$ the percent the schedule was missed for project $i$
                \end{itemize}
                
                Once all the $\Delta_i$'s have been determined, a distribution must
                be fit to the data. There are several techniques for testing the fit
                of a distribution: histograms, chi-square, Kolmogorov-Smirnov, Shapiro-Wilk,
                or Anderson-Darling \cite{Damodaran2015,Kutner2003}.
                The distribution is needed for the Cumulative
                Distribution Function (C.D.F.). The C.D.F. maps the values
                to a percentile rank within the distribution \cite{Downey2011}.
                The C.D.F. will be transformed to create
                the schedule score for CRI. Since all C.D.F. functions fall within the
                range $[0,1]$, the function needs to be shifted to center around 0, and then
                doubled to fill the desired range of $[-1,1]$. Thus the CRI schedule score
                for each project becomes the following.
                
                \[
                    S_{4_i} = 2k \cdot (C.D.F.(\Delta_i) - \frac{1}{2})
                \]
                
                Then the overall schedule score is calculated as below.
                
                \[
                    S_{4} = \sum\limits^n_{i=1} w'_i S_{4_i} \text{ where } \sum\limits^n_{i=1} w'_i = 1
                \]
    
            \paragraph{ALTERNATE APPROACH}
                An alternative approach for scoring schedule goes as follows.
                The best possible score should be achieved
                when meeting the estimated date exactly.  The maximum score
                should come from the best estimate.  Then given historical
                release data, it is easy to determine an 
                average $\Delta$ between the actual and the estimated.  
                Finishing a project within that $\Delta$ should result 
                in a positive score.  Outside the $\Delta$ results in
                negative scores.  For example, a project releasing
                one day early or one day late would receive the same score
                because in both cases the estimate was missed by one day.
                
                The first step of the formula is finding the percentage the schedules were missed for 
                historical projects.  The calculation treats over- and under-estimating the schedule
                the same.  The same penalty is applied in both cases.  For example, being 15\% late
                will result in the same score as being 15\% early. Perform this calculation
                only for projects that did not exactly meet the estimated finish date.
                
                \begin{displaymath}
                    \Delta_i = \left| \frac{F_{a_i} - F_{s_i}}{ F_{s_i} - B_{s_i} + 1} \right|
                \end{displaymath}
                
                Find the average of the $\Delta_i$'s.  This is the average percent of a missed
                schedule.  
                \begin{displaymath}
                   \bar{\Delta}  = \frac{\sum^n_{i=1}\Delta_i}{n}
                \end{displaymath}
                
                \textit{The formula for schedule is then a percentage above or below the $\Delta$.  
                The number is calculated for each project, and then averaged to form the schedule score.}
                
                After the $\bar{\Delta}$ is calculated, the following formulas are used to create the schedule scores
                for each project and then the averaged schedule score.
            
                \begin{displaymath}
                   S_{4_i} = \left\{
                     \begin{array}{lr}
                        \text{where } \Delta_i \geq 1 & : -1 \times k \\
                       \text{where }  \Delta_i \leq \bar{\Delta} & : \frac{\bar{\Delta} - \Delta_i}{\bar{\Delta}}   \times k  \\
                       \text{where } \Delta_i > \bar{\Delta} & : \frac{\bar{\Delta} - \Delta_i}{1 - \bar{\Delta}} \times k
                     \end{array}
                   \right] \text{   , calculate schedule score for each project $i$}
                \end{displaymath} 
        
                \[
                    S_{4} = \sum\limits^n_{i=1} w'_i S_{4_i} \text{ where } \sum\limits^n_{i=1} w'_i = 1 \text{   , average the schedule scores}
                \]
                where
                \begin{itemize}
                    \item $n$ is the number of projects
                    \item $F_{a_i}$ the actual finish date of project $i$
                    \item $F_{s_i}$ the scheduled finish date of project $i$
                    \item $B_{s_i}$ the scheduled beginning date of project $i$
                    \item $\Delta_i$ the percent the schedule was missed
                    \item $\bar{\Delta}$ is the average percent schedules are missed
                    \item $S_{4_i}$ is the schedule score for project $i$
                \end{itemize}
                Again, this was just an alternate approach to scoring schedule.  It will not be used
                in the case studies.

        \subsubsection{REQUIREMENTS}
            The requirements of a software development organization are important.
            
            
            \begin{description}
              \item[Function Points] find a definition and a comparison with story points
              \item[Story Points] Look in some agile book
            \end{description}
            
            \paragraph{REQUIREMENTS DATA}
            
                \begin{longtable}{@{}l rr rr}
                    \toprule%
                     \centering%
                     {\bfseries Column Name}
                     & {\bfseries Data Type}
                     &  \\
                    
                    \cmidrule[0.4pt](r{0.125em}){1-1}%
                    \cmidrule[0.4pt](lr{0.125em}){2-2}%
                    \cmidrule[0.4pt](l{0.125em}){3-3}%
                    % \midrule
                    \endhead
                    
                    Project ID & String  & Required \\
                    \myrowcolour%
                    Frequency Date & Date & Required \\
                    Requirements Scheduled & Integer & Required \\
                    \myrowcolour%
                    Actual Requirements Released & Integer  & Required \\
                    
                    \bottomrule
                    
                    \caption{REQUIREMENTS DATA NEEDED FOR CRI}
                    \label{tab:req}
                \end{longtable}
            
            \paragraph{REQUIREMENTS FORMULA}
                Requirements are desired new features or enhancements 
                to a software product.  It is important to know how many requirements
                were scheduled to be completed versus how many actually got completed.  
                \[
                    S_5 = \frac{\sum^n_{i=1}\left( R_{a_i} - R_{e_i} \right)}{n}
                \]
                \begin{itemize}
                    \item $R_a$ actual requirements completed for application $i$
                    \item $R_e$ expected requirements completed for application $i$
                \end{itemize}
            

        \subsubsection{OVERALL CRI SCORE}
            In order to accomplish the single number score that CRI requires,
            the 5 element scores must be combined. The combination of the scores
            is a weighted average.  The weights can be set based upon the 
            priority of the SDO.  Thus, the overall CRI score is calculated as below.
            
            \[
                CRI =\sum\limits^n_{i=1} w_i S_i \text{ where } \sum\limits^n_{i=1} w_i = 1
            \]


    \subsection{CORRELATIONS IN CRI}
        It is possible that 2 or more of the 5 elements of CRI will be correlated.  This means that
        one of the elements can be predicted based upon the values of the other elements.  Although
        it is possible for correlation to occur between any of the elements, the satisfaction element
        is an obvious element which deserves attention due to the human involvement of the surveys. 
        If a schedule is missed or an important requirement dropped, that could have a large negative
        effect on the satisfaction surveys.  The same could be said of quality or availability with regard to 
        the satisfaction.  However, satisfaction is not the only potentially correlated element.  It is also
        possible that a decrease in quality could result in unexpected downtime which could have a
        negative result on availability.  Similarly, if requirements are added, it is possible the schedule
        will be negatively impacted.  Also, if requirements are dropped, the quality might suffer due
        to missing functionality.  
        
        It is impossible to know which or if correlations will always exist.  Thus it is necessary to
        check for correlations after determining element and overall CRI scores.  
        If correlation is determined, the element should not be dropped but rather be weighted less than
        the other elements.  This technique keeps the most data available but lessens the importance of
        the correlated element. 
        
    \subsection{SENSITIVITY OF CRI}
    \label{sub:sensitivity}
        
        Some simulations will be run with data from each given distribution.  Then the CRI scores
        will be analyzed.
        For more on sensitivity analysis in statistical modeling, see \cite{Saltelli2000}.

    \subsection{CRI COMPARED}
        CRI is one way to evaluate an SDO, and it is also a technique of
        software analytics.  Therefore, it is beneficial to compare CRI
        with some of the other techniques and guidelines available. The
        next sections will provide those comparisons.
    
        \subsubsection{CRI VS. FOCUS AREAS OF SOFTWARE ANALYTICS}
    
            Earlier, in the introduction section \ref{subsub:softwareanalytics}, 3 main focus 
            areas  for software
            analytics were presented.  Table \ref{tab:focusareas} provides a explanation of how CRI addresses
            each focus area.  
            As can be seen, CRI clearly addresses the 3 main focus areas of software analytics.  CRI does not provide
            any mechanisms for improving the focus areas, but it provides a consistent mechanism to measure
            the focus areas. 
            
            \begin{longtable}{p{3cm}p{11cm}}
                \toprule%
                 \centering%
                 {\bfseries Focus Area}
                 & {\bfseries Why CRI?} \\
                
                \cmidrule[0.4pt](r{0.125em}){1-1}%
                \cmidrule[0.4pt](lr{0.125em}){2-2}%
                % \midrule
                \endhead
                
                User Experience & One of the 5 elements of CRI is satisfaction.  While not all of the questions focus solely on the user experience, the entire purpose of the survey is to determine if the user is satisfied
                with the software product. Does it have the correct features? Are new features added in a timely manner? Of course, specific survey questions can be created to focus solely on a certain user experience. \\
                \myrowcolour%
                Quality & Again, one of the 5 elements specifically focuses on quality.  CRI provides a single number
                to measure quality.  Therefore, it is easy to track changes in quality over time.  CRI does not address
                how to improve the quality, but without a consistent measurement, it would be impossible to determine the change in quality. \\
                Development Productivity & The combination of CRI elements, schedule and requirements, provide an 
                indication of development productivity. The schedule element measures the productivity
                related to estimated schedule. Similarly, the requirement element measures the amount of
                work actually being completed.  \\
                
                \bottomrule
                
                \caption{SOFTWARE ANALYTICS FOCUS AREAS AND CRI}
                \label{tab:focusareas}
            \end{longtable}
        
        
        \subsubsection{CRI VS. FOCUS AREAS OF SOFTWARE ANALYTICS}
            Also, section \ref{subsub:softwareanalytics} mentions 3 important questions that
            software analytics must address.  Table \ref{tab:questions} presents the 3 questions
            and a description of how CRI addresses that specific question.  It is clear
            that CRI addresses the questions.  CRI is a beneficial technique of software
            analytics when applied to software development organizations.
        
            \begin{longtable}{p{4cm}p{10cm}}
                
                \toprule%
                 \centering%
                 {\bfseries Question}
                 & {\bfseries Why CRI?} \\
                
                \cmidrule[0.4pt](r{0.125em}){1-1}%
                \cmidrule[0.4pt](lr{0.125em}){2-2}%
                % \midrule
                \endhead
                
                How much better is my model performing than a naive strategy, such as guessing? & CRI 
                provides consistency which may not exist without it.  Therefore, CRI removes the 
                guesswork of measuring a software development organization. \\
                \myrowcolour%
                How practically significant are the results? & The CRI score is consistent
                and easy to comprehend.  Thus comparison with past performance is quick 
                and simple.  This is a significant advantage for software 
                development organizations. \\
                How sensitive are the results to small changes in one or more of the inputs? & 
                The question was extensively addressed in section \ref{sub:sensitivity}.  It appears CRI is
                not overly sensitive to small changes in the inputs.\\
                
                \bottomrule
                
                \caption{IMPORTANT QUESTIONS FOR SOFTWARE ANALYTICS AND CRI}
                \label{tab:questions}
            \end{longtable}

        
        \subsubsection{CRI VS. BALANCED SCORECARD}
            Section \ref{sec:bsc} discusses the characteristics of the 
            balanced scorecard.  Table \ref{tab:bsc} presents a
            comparison of the characteristics of a balanced scorecard
            versus CRI.
            As the newer 2 characteristics of a balanced scorecard
            have only existed since 2010 and the adoption is limited,
            the comparison will only be against the original 4 balanced
            scorecard characteristics.
            
             \begin{longtable}{p{3cm}p{1cm}p{10cm}}
                % pairs: absolute number (percentage)
                
                \toprule%
                 \centering%
                 {\bfseries Balanced Scorecard}
                 & {\bfseries CRI?} 
                 & {\bfseries Explanation} \\
                
                \cmidrule[0.4pt](r{0.125em}){1-1}%
                \cmidrule[0.4pt](lr{0.125em}){2-2}%
                \cmidrule[0.4pt](lr{0.125em}){3-3}%
                % \midrule
                \endhead
                
                Financial & No & CRI does not address financial as it is best suited 
                    for an organization that treats software development as a fixed,
                    budgeted expense. If the budget is fixed, CRI provides a number
                    to indicate the amount of value for that fixed budget.  \\
                \myrowcolour%
                Customer Focus & Yes & CRI includes a customer survey which is 
                    completely customer focused.\\
                Internal Process & Yes & CRI is highly focused on internal processes.
                    The CRI elements of schedule and requirement are completely
                    focused on how reality meets the expected process. CRI is
                    negatively impacted when internal processes are followed. \\
                \myrowcolour%
                Learning/Growth & No & CRI does not address this characteristic. \\
                
                \bottomrule
                
                \caption{BALANCED SCORECARD VERSUS CRI}
                \label{tab:bsc}
            \end{longtable}

        
        \subsubsection{CRI VS. PROJECT MANAGEMENT MEASUREMENT}
            Previously in Section \ref{sec:pm}, the project management
            measurement was presented.   Its greatest limitation is the lack of
            focus on the entire SDO.  Project management measurement says 
            nothing about the availability of the software infrastructure
            or the satisfaction of the users.  It is not near as comprehensive
            as either the balanced scorecard or CRI. Actually, CRI incorporates
            the 5 core measurements from project management.
            Plus, the process productivity
            number is less suitable for upper management and more
            suitable for project teams.  Although it does provide a single
            process productivity number, it is does not have the same focus
            as CRI.
            
            
            
            
\end{document}