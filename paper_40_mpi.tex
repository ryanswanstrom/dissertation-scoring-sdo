\documentclass[SDSUThesis.tex]{subfiles} 
\begin{document}

\section{MASTER PERFORMANCE INDICATOR (MPI)}

    Software development organizations struggle to measure overall performance.  
    The Master Performance Indicator (MPI) is an algorithm to provide a single 
    number score to measure the performance of a software development organization. 
    It works by statistically analyzing the past performance of the 
    organization and using that information to score an organization 
    on current performance.  MPI focuses on the following 
    elements of a software development organization: \textit{quality}, 
    \textit{availability}, \textit{satisfaction}, \textit{schedule},
    and \textit{requirements}. A separate MPI
    score is calculated for each element and 
    then aggregated together to form an overall 
    MPI score.  A new MPI score will be calculated based
    upon the selection of a given time period (weekly, monthly, quarterly).
    \textbf{MPI is not meant to be comparative between
    organizations, but to measure the amount of increase or decrease
    a single organization exhibits across elements}.  MPI is made to be easily 
    expandable to other elements if desired.
    
    The scores for MPI will range from -1, indicating the worst 
    performance, all the way to +1, indicating perfection.
    A score of 0 is an indication of meeting the basic expectations. 
    A negative score indicates under-performance and a 
    positive score indicates over-performance.   Here are some 
    examples. An MPI score of 0.35 means the organization is 
    performing 35\% better than expected.  Conversely, a score 
    of -0.15 means an organization is performing 15\% worse than expected.  

    Below are the attributes of the MPI scoring.
    \begin{itemize}
        \item The range of scores must have equal values above and below 0
        \item The minimum score must equate to the worst possible 
            performance, however that is defined
        \item Similarily, the maximum score must equate to the best possible performance.  
        \item A score of 0 must be average (or expected) performance
        \item All individual elements must have the same scoring range
    \end{itemize}
            
    As long as those 5 features are met, the range can be anything.  
    The range of $[-1,1]$ was chosen because it is easy to scale 
    to a different range such as $[-10,10]$ or $[-100,100]$.  
    It maybe makes sense to use variables for the range.  
    The formulas are designed to fall in the range $[-100,100]$.  
    Thus scaling can be applied to obtain values in any appropriate range. 
    The scaling is denoted with the variable $k$.  The scale will be the same
    for all 5 elements.
    
    \subsection{ELEMENTS OF MPI}
        The MPI score consists of data collected from five
        different elements of an SDO.  
        \begin{enumerate}
            \item Quality
            \item Availability
            \item Satisfaction
            \item Schedule
            \item Requirements
        \end{enumerate}
        These five elements will be outline in the next sections.
        
        \subsubsection{QUALITY}
            Measuring quality is a crucial part of accessing 
            software development results.  
            Poor quality means time, money, and resources are spent 
            fixing the problems. As a result, new features are not being
            created. One of the key indicators of software quality 
            is defects.  Thus, it is important
            to measure the number of defects associated with a software 
            release.  The following is the necessary data to collect
            and an algorithm to calculate
            the quality score of software being released. 
            
            \paragraph{QUALITY DATA}
                In order to properly score the quality of an SDO,
                certain data needs to be obtained in order to measure
                performance. Table \ref{tab:qualitydata} identifies
                the columns of data that will be used to create
                a score for the quality element of MPI.  
                Each column is classified as \textit{required} or
                \textit{optional}.  That is to allow some flexibility
                in the model for organizations that collect
                varying amounts of data.
                
                \begin{longtable}{@{}l rr rr}
                    % pairs: absolute number (percentage)
                    
                    \toprule%
                     \centering%
                     {\bfseries Column Name}
                     & {\bfseries Data Type}
                     &  \\
                    
                    \cmidrule[0.4pt](r{0.125em}){1-1}%
                    \cmidrule[0.4pt](lr{0.125em}){2-2}%
                    \cmidrule[0.4pt](l{0.125em}){3-3}%
                    % \midrule
                    \endhead
                    
                    Application ID & String (factor) & Required \\
                    \myrowcolour%
                    Frequency Date & Date & Required \\
                    Development Effort & Integer & Required \\
                    \myrowcolour%
                    Testing Effort & Integer & Optional \\ 
                    SIT Defects & Integer & Optional \\ 
                    \myrowcolour%
                    UAT Defects & Integer  & Optional \\ 
                    PROD Defects & Integer  & Required \\
                    
                    \bottomrule
                    
                    \caption{QUALITY DATA NEEDED FOR MPI}
                    \label{tab:qualitydata}
                \end{longtable}
                
                The development and testing effort can come from any of 
                the following choices for effort. It is possible
                that other measures will work for effort.
                \begin{description}
                    \item[Actual Time] This number is a representation of the 
                        total amount of time spent on a project.  This number
                        can be measured in any unit of time: hours, days, weeks,
                        etc.  Actual time can be applied to development or testing
                        effort.
                    \item[Estimated Time] This number is a representation of the 
                        initial estimated amount of time spent on a project.  This number
                        can be measured in any unit of time: hours, days, weeks,
                        etc.  Estimated time can be applied to development or testing
                        effort.  It is common for the estimated and actual times
                        to be diffent.
                    \item[Source Lines Of Code (SLOC)]  This number is the count of 
                        the total number of lines of source code for a project. 
                        Obviously, 
                        this item only counts as a level of
                        effort for development unless coding in used to generate
                        automated test cases. \footnote{Automated testing is the
                        process of creating software to automatically run tests
                        against other software.  The adoption of automated testing
                        is varied and it is not a solution in all cases 
                        \cite{Ramler2006}. }
                    \item[Modified Lines Of Code] This number is a count of the number 
                        of modified lines of source code. Modified lines is 
                        defined as the number of deleted, added, and modified 
                        lines of source code.  This number is different from above
                        since it does not include all the lines of source code.  Similar
                        to above, this number makes more sense for development effort.
                    \item[Test Cases] A test case is a step or series of steps followed
                        to validate some expected outcome of software.  Organizations
                        will create a number of test cases to be validated for a
                        software system.  The number of such test cases could
                        be used as a level of testing effort.
                \end{description}
            
            \paragraph{QUALITY FORMULA}
                The first step in creating a score for the quality
                element is analysis of the historical data.  The historical
                data is all quality data collected before  a given point
                in time.  Some common historical cutoffs are the current date
                or the end of the previous fiscal year.  Then a mathematical
                model to predict PROD Defects will be produced.  In statistical
                terms, the response is \textit{PROD Defects} and the predictors
                are: \textit{UAT Defects}, \textit{SIT Defects}, 
                \textit{Testing Effort}, and \textit{Development Effort}.  
                Some of the following strategies will be employed to find 
                a reasonable model.
                \begin{itemize}
                    \item Removal of outliers and/or influential points
                    \item Linear Regression
                    \item Stepwise Regression
                    \item Ridge Regression for suspected multicolinearity
                \end{itemize}
                
                Once a model has been found, it will be labeled as $f$ and it
                will not change.  The function $f$ can be the same for all 
                Application IDs or it can be different for each Application ID
                or any combination of Application IDs.   It serves as the 
                quality baseline for MPI. 
                All future quality scores will be dependent upon the original
                $f$.  Once set, the model will not change.
                
                After the model $f$ has been determined, it is time
                to calculate the quality score for each application ID
                within the given time period.  The quality score
                for each Application ID can be calculated as follows.
                
                \begin{displaymath}
                   S_{1_i} = \left\{
                     \begin{array}{lr}
                       \text{where } f_i \geq d_i & :  \frac{f_i - d_i}{f_i} \times k  \\
                       \text{where } d_i > f_i  & : \frac{f_i-d_i }{6\sigma_i} \times k
                     \end{array}
                   \right] \text{   , calculate quality score for each app $i$}
                \end{displaymath} 
                where
                \begin{itemize}
                    \item $S_{1_i}$ is the quality score for Application ID $i$
                    \item $n$ is the number of Application IDs
                    \item $d_i$ is the actual PROD defects
                    \item $f_i$ is the function to predict PROD defects for Application $i$ based upon 
                        \textit{UAT Defects}, \textit{SIT Defects}, \textit{Testing Effort}, 
                        and \textit{Development Effort}
                    \item $6\sigma$ 6 times an estimate of the standard deviation of the 
                        population using function $f_i$ above
                \end{itemize}
                
                \[
                    S_{1} = \sum\limits^n_{i=1} w'_i S_{1_i} \text{ where } \sum\limits^n_{i=1} w'_i = 1
                \]
                where
                \begin{itemize}
                    \item $S_1$ is the combined quality score for all Application IDs, 
                    a weighted average
                \end{itemize}
                
                Then $S_1$ represents the quality score for that given time period.
        


        \subsubsection{AVAILABILITY}
            All the new requirements and great quality do not matter if the software is not available.  
            The following algorithm will calculate the score of software availability.
            \begin{displaymath}
               S_{3_i} = \left\{
                 \begin{array}{lr}
                   \text{where } A_{a_i} \leq A_{e_i} & : \left[ \frac{A_{a_i} - A_{e_i}}{A_{e_i}} \right] \\
                   \text{where } A_{a_i} > A_{e_i}  & : \left[ \frac{A_{a_i} - A_{e_i} }{1-A_{e_i}} \right]
                 \end{array}
               \right.
            \end{displaymath} 
            \[
                S_3 = \frac{\sum^n_{i=1}S_{3_i}}{n}
            \]
            
            \begin{itemize}
            \item $A_{a_i}$ actual availability for app i
            \item $A_{e_i}$ expected availability for app i
            \end{itemize}
            
            \begin{longtable}{@{}l rr rr}
            % pairs: absolute number (percentage)
            
            \toprule%
             \centering%
             {\bfseries Column Name}
             & {\bfseries Data Type}
             &  \\
            
            \cmidrule[0.4pt](r{0.125em}){1-1}%
            \cmidrule[0.4pt](lr{0.125em}){2-2}%
            \cmidrule[0.4pt](l{0.125em}){3-3}%
            % \midrule
            \endhead
            
            Application ID & String  & Required \\
            \myrowcolour%
            Frequency Date & Date & Required \\
            Uptime & Float & Optional \\
            \myrowcolour%
            Scheduled Downtime & Float & Optional \\
            Unscheduled Downtime & Float  & Optional \\
            \myrowcolour%
            Percent Uptime & Float & Required \\
            Expected Percent Uptime & Float & Required \\
            
            \bottomrule
            
            \caption{AVAILABILITY DATA NEEDED FOR MPI}
            \label{tab:avail}
            \end{longtable}

        \subsubsection{SATISFACTION}
            The satisfaction of the customer is also very important.  
            The following is an algorithm to calculate the satisfaction of software.
            \[
                S_2 = \frac{\sum^n_{i=1}\left( \frac{\sum^m_{j=1}a_{ij}- \frac{min + max}{2}}{m}  \right)}{n}
            \]
            
            \begin{itemize}
            \item $a_{i j}$ answer to question $j$ for app $i$
            \item $m$ number of questions with equal weights
            \item $n$ number of apps
            \item $min$ minimum score for a question
            \item $max$ maximum score for a question
            \end{itemize}
            
            \begin{longtable}{@{}l rr rr}
            % pairs: absolute number (percentage)
            
            \toprule%
             \centering%
             {\bfseries Column Name}
             & {\bfseries Data Type}
             &  \\
            
            \cmidrule[0.4pt](r{0.125em}){1-1}%
            \cmidrule[0.4pt](lr{0.125em}){2-2}%
            \cmidrule[0.4pt](l{0.125em}){3-3}%
            % \midrule
            \endhead
            
            Question ID & String  & Required \\
            \myrowcolour%
            Question Text & String  & Optional \\
            Application ID & String & Required \\
            \myrowcolour%
            Frequency Date & Date & Required \\
            Response & Integer between min and max  & Required \\
            \myrowcolour%
            Response Date & Date & Optional \\
            
            \bottomrule
            
            \caption{SATISFACTION DATA NEEDED FOR MPI}
            \label{tab:satisfaction}
            \end{longtable}


        \subsubsection{SCHEDULE}  
            Delivery of software in a timely manner is an essential part of being a 
            successful SDO.  Being able to meet scheduled deadlines is a sign of accurate
            estimation and planning.  Drastically missing deadlines is a sign of an SDO
            with a process that needs refinement.  Studies have shown that 
            software projects exceed the estimates by an average of
            30\% \cite{Jorgensen2014}.  Thus it is important to score SDOs on accurate
            schedule adherence. Without tracking and measuring schedule adherence, it will
            not improve.  
            
            The score is based upon the historical deviance of estimates for
            projects.  The top score of
            100 will be obtained when the schedule is met exactly.  A historical average 
            of deviance of schedule will be used to determine an appropriate range
            of schedule adherence.  A schedule that occurs within that range will be awarded
            a score of 0 or above, with less deviance being awarded more points.  Schedules
            that do not fall within the range of deviance will be awarded negative scores, 
            with a greater deviance resulting in a lower score all the way to -100 which means
            the package took twice as long as expected or more.
            For example, a package that takes more than twice as long to deliver than
            estimated will receive a score of -100.
            
            \paragraph{SCHEDULE DATA}
            
                \begin{longtable}{@{}l rr rr}
                    \toprule%
                     \centering%
                     {\bfseries Column Name}
                     & {\bfseries Data Type}
                     &  \\
                    
                    \cmidrule[0.4pt](r{0.125em}){1-1}%
                    \cmidrule[0.4pt](lr{0.125em}){2-2}%
                    \cmidrule[0.4pt](l{0.125em}){3-3}%
                    % \midrule
                    \endhead
                    
                    Application ID & String  & Required \\
                    \myrowcolour%
                    Frequency Date & Date & Required \\
                    Scheduled Start Date & Date & Required \\
                    \myrowcolour%
                    Scheduled Finish Date & Date & Required \\
                    Actual Start Date & Date  & Required \\
                    \myrowcolour%
                    Actual Finish Date & Date  & Required \\
                    
                    \bottomrule
                    
                    \caption{SCHEDULE DATA NEEDED FOR MPI}
                    \label{tab:sched}
                \end{longtable}
                    
            \paragraph{SCHEDULE FORMULA}
    
                The formula for schedule is based upon past performance.  Of the 422 projects
                scheduled since June 2013, only 45 projects had an estimated start date, estimated
                finish date and an actual finish date.  Of those 45 projects; 20 finished exactly on time,
                9 finished early and 16 finished late.  
                
                The first step of the formula is finding the percentage the schedules were missed for 
                historical projects.  The calculation treats over- and under-estimating the schedule
                the same.  The same penalty is applied in both cases.  For example, being 15\% late
                will result in the same score as being 15\% early. Perform this calculation
                only for projects that did not exactly meet the estimated finish date.
                
                \begin{displaymath}
                    \Delta_i = \left| \frac{F_{a_i} - F_{s_i}}{ F_{s_i} - B_{s_i} + 1} \right|
                \end{displaymath}
                
                Find the average of the $\Delta_i$'s.  This is the average percent of a missed
                schedule.  
                \begin{displaymath}
                   \bar{\Delta}  = \frac{\sum^n_{i=1}\Delta_i}{n}
                \end{displaymath}
                
                \textit{The formula for schedule is then a percentage above or below the $\Delta$.  
                The number is calculated for each project, and then averaged to form the schedule score.}
                
                After the $\bar{\Delta}$ is calculated, the following formulas are used to create the schedule scores
                for each project and then the averaged schedule score.
            
                \begin{displaymath}
                   S_{4_i} = \left\{
                     \begin{array}{lr}
                        \text{where } \Delta_i \geq 1 & : -1 \times 100 \\
                       \text{where }  \Delta_i \leq \bar{\Delta} & : \frac{\bar{\Delta} - \Delta_i}{\bar{\Delta}}   \times 100  \\
                       \text{where } \Delta_i > \bar{\Delta} & : \frac{\bar{\Delta} - \Delta_i}{1 - \bar{\Delta}} \times 100
                     \end{array}
                   \right] \text{   , calculate schedule score for each project $i$}
                \end{displaymath} 
        
                \[
                    S_{4} = \frac{\sum^n_{i=1} S_{4_i}}{n} \text{   , average the schedule scores}
                \]
                \begin{itemize}
                    \item $n$ is the number of projects
                    \item $F_{a_i}$ the actual finish date of project i
                    \item $F_{s_i}$ the scheduled finish date of project i
                    \item $B_{s_i}$ the scheduled beginning date of project i
                    \item $\Delta_i$ the percent the schedule was missed
                    \item $\bar{\Delta}$ is the average percent RT misses schedules, $\approx 26$
                    \item $S_{4_i}$ is the schedule score for project $i$
                \end{itemize}
    
    
            \paragraph{ALTERNATE APPROACH}
                The best possible score should be achieved
                when meeting the estimated date exactly.  The maximum score
                should come from the best estimate.  Then given historical
                release data, it is easy to determine an 
                average $\Delta$ between the actual and the estimated.  
                Finishing a project within that $\Delta$ should result 
                in a positive score.  Outside the $\Delta$ results in
                negative scores.  For example, a project releasing
                one day early or one day late would receive the same score
                because in both cases the estimate was missed by one day.

%The score can be accomplished with the following 
%algorithm.
%\[
%    S_4 = \frac{\sum^n_{i=1} \left( C_{a_i} - C_{e_i} \right)}{n}
%\]
%\begin{itemize}
%\item $C_a$ actual number of days for application $i$
%\item $C_e$ estimated number of days for application $i$
%\end{itemize}


        \subsubsection{REQUIREMENTS}
            
            \paragraph{REQUIREMENTS DATA}
            
                \begin{longtable}{@{}l rr rr}
                    \toprule%
                     \centering%
                     {\bfseries Column Name}
                     & {\bfseries Data Type}
                     &  \\
                    
                    \cmidrule[0.4pt](r{0.125em}){1-1}%
                    \cmidrule[0.4pt](lr{0.125em}){2-2}%
                    \cmidrule[0.4pt](l{0.125em}){3-3}%
                    % \midrule
                    \endhead
                    
                    Application ID & String  & Required \\
                    \myrowcolour%
                    Frequency Date & Date & Required \\
                    Requirements Scheduled & Integer & Required \\
                    \myrowcolour%
                    Actual Requirements Released & Integer  & Required \\
                    
                    \bottomrule
                    
                    \caption{REQUIREMENTS DATA NEEDED FOR MPI}
                    \label{tab:req}
                \end{longtable}
            
            \paragraph{REQUIREMENTS FORMULA}
                Requirements are desired new features or enhancements 
                to a software product.  It is important to know how many requirements
                were scheduled to be completed versus how many actually got completed.  
                \[
                    S_5 = \frac{\sum^n_{i=1}\left( R_{a_i} - R_{e_i} \right)}{n}
                \]
                \begin{itemize}
                    \item $R_a$ actual requirements completed for application $i$
                    \item $R_e$ expected requirements completed for application $i$
                \end{itemize}
            

        \subsubsection{OVERALL MPI SCORE}
            One complete overall score cannot be successful without including 
            information from various aspects of the software development 
            organization.  Therefore, the previous results
            are combined to provide one overall score.
            \[
                MPI =\sum\limits^n_{i=1} w_i S_i \text{ where } \sum\limits^n_{i=1} w_i = 1
            \]


    \subsection{SENSITIVITY OF MPI}
    \label{sub:sensitivity}
        
        Some simulations will be run with data from each given distribution.  Then the MPI scores
        will be analyzed.
        For more on sensitivity analysis in statistical modeling, see \cite{Saltelli2000}.

    \subsection{IMPORTANCE OF MPI}
    
        Earlier, in the introduction section \ref{subsub:softwareanalytics}, 3 main focus 
        areas and 3 important questions for software
        analytics were presented.  Table \ref{tab:focusareas} provides a explanation of how MPI addresses
        each focus area.  
        As can be seen, MPI clearly addresses the 3 main focus areas of software analytics.  MPI does provide
        any mechanisms for improving the focus areas, but it provides a consistent mechanism to measure
        the focus areas. 
        
        \begin{longtable}{p{3cm}p{11cm}}
            % pairs: absolute number (percentage)
            
            \toprule%
             \centering%
             {\bfseries Focus Area}
             & {\bfseries Why MPI?} \\
            
            \cmidrule[0.4pt](r{0.125em}){1-1}%
            \cmidrule[0.4pt](lr{0.125em}){2-2}%
            % \midrule
            \endhead
            
            User Experience & One of the 5 elements of MPI is satisfaction.  While not all of the questions focus solely on the user experience, the entire purpose of the survey is to determine if the user is satisfied
            with the software product. Does it have the correct features? Are new features added in a timely manner? Of course, specific survey questions can be created to focus solely on a certain user experience. \\
            \myrowcolour%
            Quality & Again, one of the 5 elements specifically focuses on quality.  MPI provides a single number
            to measure quality.  Therefore, it is easy to track changes in quality over time.  MPI does not address
            how to improve the quality, but without a consistent measurement, it would be impossible to determine the change in quality. \\
            Development Productivity & The combination of MPI elements, schedule and requirements, provide an 
            indication of development productivity. The schedule element measures the productivity
            related to estimated schedule. Similarly, the requirement element measures the amount of
            work actually being completed.  \\
            
            \bottomrule
            
            \caption{SOFTWARE ANALYTICS FOCUS AREAS AND MPI}
            \label{tab:focusareas}
        \end{longtable}
        
        
        \begin{longtable}{p{4cm}p{10cm}}
            % pairs: absolute number (percentage)
            
            \toprule%
             \centering%
             {\bfseries Question}
             & {\bfseries Why MPI?} \\
            
            \cmidrule[0.4pt](r{0.125em}){1-1}%
            \cmidrule[0.4pt](lr{0.125em}){2-2}%
            % \midrule
            \endhead
            
            How much better is my model performing than a naive strategy, such as guessing? & MPI 
            provides consistency which may not exist without it.  Therefore, MPI removes the 
            guesswork of measuring a software development organization. \\
            \myrowcolour%
            How practically significant are the results? & The MPI score is consistent
            and easy to comprehend.  Thus comparison with past performance is quick 
            and simple.  This is a significant advantage for software 
            development organizations. \\
            How sensitive are the results to small changes in one or more of the inputs? & 
            The question was extensively addressed in section \ref{sub:sensitivity}.  It appears MPI is
            not overly sensitive to small changes in the inputs.\\
            
            \bottomrule
            
            \caption{IMPORTANT QUESTIONS FOR SOFTWARE ANALYTICS AND MPI}
            \label{tab:questions}
        \end{longtable}

\end{document}