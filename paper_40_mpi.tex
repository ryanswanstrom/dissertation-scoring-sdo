\documentclass[SDSUThesis.tex]{subfiles} 
\begin{document}

\section{ELEMENTS OF MPI}

Software development organizations struggle to measure overall performance.  The Master Performance Indicator (MPI) is an algorithm to provide a single number score for the performance of a software development organization. It works by statistically analyzing the past performance of the organization and using that information to score an organization on current and future performance.  MPI focuses on the following elements of a software development organization: \textit{requirements},  \textit{quality},  \textit{on-schedule},  \textit{availability}, and  \textit{satisfaction}. MPI is not meant to be comparative between organizations, but to measure the amount of increase or decrease a single organization exhibits.
It is possible the concept could be expanded beyond just software development organizations or the previously stated elements. 

Here are the attributes of the MPI scoring.
            \begin{itemize}
                \item The range of scores must have equal values above and below 0
                \item The minimum score must equate to the worst possible performance, however that is defined
                \item Similarily, the maximum score must equate to the best possible performance.  
                \item A score of 0 must be average (or expected) performance
                \item All individual elements must have the same scoring range
            \end{itemize}
            
            As long as those 5 features are met, the range can be anything.  The range of $[-1,1]$ was chosen because it is easy to scale to a different range such as $[-10,10]$ or $[-100,100]$.  It maybe makes sense to use variables for the range.  The formulas are designed to fall in the range $[-1,1]$.  Thus scaling can be applied to obtain values in any appropriate range. 
            
\subsection{AVAILABILITY}
All the new requirements and great quality do not matter if the software is not available.  
The following algorithm will calculate the score of software availability.
\begin{displaymath}
   S_{3_i} = \left\{
     \begin{array}{lr}
       \text{where } A_{a_i} \leq A_{e_i} & : \left[ \frac{A_{a_i} - A_{e_i}}{A_{e_i}} \right] \\
       \text{where } A_{a_i} > A_{e_i}  & : \left[ \frac{A_{a_i} - A_{e_i} }{1-A_{e_i}} \right]
     \end{array}
   \right.
\end{displaymath} 
\[
    S_3 = \frac{\sum^n_{i=1}S_{3_i}}{n}
\]

\begin{itemize}
\item $A_{a_i}$ actual availability for app i
\item $A_{e_i}$ expected availability for app i
\end{itemize}

\begin{longtable}{@{}l rr rr}
% pairs: absolute number (percentage)

\toprule%
 \centering%
 {\bfseries Column Name}
 & {\bfseries Data Type}
 &  \\

\cmidrule[0.4pt](r{0.125em}){1-1}%
\cmidrule[0.4pt](lr{0.125em}){2-2}%
\cmidrule[0.4pt](l{0.125em}){3-3}%
% \midrule
\endhead

Application ID & String  & Required \\
\myrowcolour%
Frequency Date & Date & Required \\
Uptime & Float & Optional \\
\myrowcolour%
Scheduled Downtime & Float & Optional \\
Unscheduled Downtime & Float  & Optional \\
\myrowcolour%
Percent Uptime & Float & Required \\
Expected Percent Uptime & Float & Required \\

\bottomrule

\caption{Availability - Inputs}
\label{tab:avail}
\end{longtable}


\subsection{QUALITY}
Quality is arguably the most important part of software development.  Poor quality means time, money, and resources are spent 
fixing the quality.  One of the key indicators of software quality is defects.  Thus, it is important
to measure the number of defects associated with a software release.  The following is an algorithm to calculate
the quality score of software being released. 


\begin{displaymath}
   S_{1_i} = \left\{
     \begin{array}{lr}
       \text{where } f \geq d_i & : \left[ \frac{f - d_i}{f} \right] \\
       \text{where } d_i > f  & : \left[ \frac{f-d_i }{6\sigma} \right]
     \end{array}
   \right]
\end{displaymath} 

\[
    S_{1} = \frac{\sum^n_{i=1} S_{1_i}}{n}
\]
\begin{itemize}
\item $n$ is the number of apps
\item $d_i$ is the actual defects
\item $f(h_i)$ is the function to predict defects based upon hours and other aspects of software development
\item $6\sigma$ 6 times an estimate of the standard deviation of the population using function $f$ above
\end{itemize}

Here are the columns of the data.

\begin{longtable}{@{}l rr rr}
% pairs: absolute number (percentage)

\toprule%
 \centering%
 {\bfseries Column Name}
 & {\bfseries Data Type}
 &  \\

\cmidrule[0.4pt](r{0.125em}){1-1}%
\cmidrule[0.4pt](lr{0.125em}){2-2}%
\cmidrule[0.4pt](l{0.125em}){3-3}%
% \midrule
\endhead

Application ID & String (factor) & Required \\
\myrowcolour%
Frequency Date & Date & Required \\
Development Hours & Integer & Required \\
\myrowcolour%
Testing Hours & Integer & Optional \\ 
Defects in SIT & Integer & Optional \\ 
\myrowcolour%
Defects in UAT & Integer  & Optional \\ 
Defects in PROD & Integer  & Required \\

\bottomrule

\caption{Quality - Inputs}
\label{tab:quality}
\end{longtable}


There are 2 parts here.  First the system needs to be loaded with this data.  Hopefully,
a file with this data will exist going back a few months or years.  It is also important
to note that the data load must be optional if an organization does not have existing
data.  Second, every release (month/year) a new file will be upload with new information
about the most recent release.

Here is the problem for loading the data:
Given a data file with the above columns, can a regression model be automatically found
that fits the data?  The regression model will serve as the "target" for future 
releases.  Here is a list of possible things to automate to get a good model.
\begin{itemize}
\item Linear Regression
\item Stepwise Regression
\item Ridge Regression for suspected multicolinearity
\item Removal of outliers
\end{itemize}


\subsection{SATISFACTION}
The satisfaction of the customer is also very important.  The following is an algorithm to calculate the satisfaction of software.
\[
    S_2 = \frac{\sum^n_{i=1}\left( \frac{\sum^m_{j=1}a_{ij}- \frac{min + max}{2}}{m}  \right)}{n}
\]

\begin{itemize}
\item $a_{i j}$ answer to question $j$ for app $i$
\item $m$ number of questions with equal weights
\item $n$ number of apps
\item $min$ minimum score for a question
\item $max$ maximum score for a question
\end{itemize}

\begin{longtable}{@{}l rr rr}
% pairs: absolute number (percentage)

\toprule%
 \centering%
 {\bfseries Column Name}
 & {\bfseries Data Type}
 &  \\

\cmidrule[0.4pt](r{0.125em}){1-1}%
\cmidrule[0.4pt](lr{0.125em}){2-2}%
\cmidrule[0.4pt](l{0.125em}){3-3}%
% \midrule
\endhead

Question ID & String  & Required \\
\myrowcolour%
Question Text & String  & Optional \\
Application ID & String & Required \\
\myrowcolour%
Frequency Date & Date & Required \\
Response & Integer between min and max  & Required \\
\myrowcolour%
Response Date & Date & Optional \\

\bottomrule

\caption{Satisfaction - Inputs}
\label{tab:satisfaction}
\end{longtable}


\subsection{SCHEDULE}  

    \subsubsection{Overview}
        Delivery of software in a timely manner is an essential part of being a 
        successful SDO.  Being able to meet scheduled deadlines is a sign of accurate
        estimation and planning.  Drastically missing deadlines is a sign of an SDO
        with a process that needs refinement.  Studies have shown that 
        software projects exceed the estimates by an average of
        30\% \cite{Jorgensen2014}.  Thus it is important to score SDOs on accurate
        schedule adherence. Without tracking and measuring schedule adherence, it will
        not improve.  
        
        The score is based upon the historical deviance of estimates for
        projects.  The top score of
        100 will be obtained when the schedule is met exactly.  A historical average 
        of deviance of schedule will be used to determine an appropriate range
        of schedule adherence.  A schedule that occurs within that range will be awarded
        a score of 0 or above, with less deviance being awarded more points.  Schedules
        that do not fall within the range of deviance will be awarded negative scores, 
        with a greater deviance resulting in a lower score all the way to -100 which means
        the package took twice as long as expected or more.
        For example, a package that takes more than twice as long to deliver than
        estimated will receive a score of -100.
    
    \subsubsection{Formula}
    
    The formula for schedule is based upon past performance.  Of the 422 projects
    scheduled since June 2013, only 45 projects had an estimated start date, estimated
    finish date and an actual finish date.  Of those 45 projects; 20 finished exactly on time,
    9 finished early and 16 finished late.  
    
    The first step of the formula is finding the percentage the schedules were missed for 
    historical projects.  The calculation treats over- and under-estimating the schedule
    the same.  The same penalty is applied in both cases.  For example, being 15\% late
    will result in the same score as being 15\% early. Perform this calculation
    only for projects that did not exactly meet the estimated finish date.
    
        \begin{displaymath}
            \Delta_i = \left| \frac{F_{a_i} - F_{s_i}}{ F_{s_i} - B_{s_i} + 1} \right|
        \end{displaymath}
        
        Find the average of the $\Delta_i$'s.  This is the average percent of a missed
        schedule.  
        \begin{displaymath}
           \bar{\Delta}  = \frac{\sum^n_{i=1}\Delta_i}{n}
        \end{displaymath}
        
        \textit{The formula for schedule is then a percentage above or below the $\Delta$.  
        The number is calculated for each project, and then averaged to form the schedule score.}
        
        After the $\bar{\Delta}$ is calculated, the following formulas are used to create the schedule scores
        for each project and then the averaged schedule score.
    
        \begin{displaymath}
           S_{4_i} = \left\{
             \begin{array}{lr}
                \text{where } \Delta_i \geq 1 & : -1 \times 100 \\
               \text{where }  \Delta_i \leq \bar{\Delta} & : \frac{\bar{\Delta} - \Delta_i}{\bar{\Delta}}   \times 100  \\
               \text{where } \Delta_i > \bar{\Delta} & : \frac{\bar{\Delta} - \Delta_i}{1 - \bar{\Delta}} \times 100
             \end{array}
           \right] \text{   , calculate schedule score for each project $i$}
        \end{displaymath} 

        \[
            S_{4} = \frac{\sum^n_{i=1} S_{4_i}}{n} \text{   , average the schedule scores}
        \]
        \begin{itemize}
            \item $n$ is the number of projects
            \item $F_{a_i}$ the actual finish date of project i
            \item $F_{s_i}$ the scheduled finish date of project i
            \item $B_{s_i}$ the scheduled beginning date of project i
            \item $\Delta_i$ the percent the schedule was missed
            \item $\bar{\Delta}$ is the average percent RT misses schedules, $\approx 26$
            \item $S_{4_i}$ is the schedule score for project $i$
        \end{itemize}
    
    
        
    \subsubsection{Example Calculation}
    \subsubsection{Data and Calculations}
    \subsubsection{Past Performance}
    \subsubsection{Process for the Future}
    
    \subsubsection{ALTERNATE APPROACH}
    The best possible score should be achieved
            when meeting the estimated date exactly.  The maximum score
            should come from the best estimate.  Then given historical
            release data, it is easy to determine an 
            average $\Delta$ between the actual and the estimated.  
            Finishing a project within that $\Delta$ should result 
            in a positive score.  Outside the $\Delta$ results in
            negative scores.  For example, a project releasing
            one day early or one day late would receive the same score
            because in both cases the estimate was missed by one day.

%The score can be accomplished with the following 
%algorithm.
%\[
%    S_4 = \frac{\sum^n_{i=1} \left( C_{a_i} - C_{e_i} \right)}{n}
%\]
%\begin{itemize}
%\item $C_a$ actual number of days for application $i$
%\item $C_e$ estimated number of days for application $i$
%\end{itemize}

\begin{longtable}{@{}l rr rr}
% pairs: absolute number (percentage)

\toprule%
 \centering%
 {\bfseries Column Name}
 & {\bfseries Data Type}
 &  \\

\cmidrule[0.4pt](r{0.125em}){1-1}%
\cmidrule[0.4pt](lr{0.125em}){2-2}%
\cmidrule[0.4pt](l{0.125em}){3-3}%
% \midrule
\endhead

Application ID & String  & Required \\
\myrowcolour%
Frequency Date & Date & Required \\
Scheduled Start Date & Date & Required \\
\myrowcolour%
Scheduled Finish Date & Date & Required \\
Actual Start Date & Date  & Required \\
\myrowcolour%
Actual Finish Date & Date  & Required \\

\bottomrule

\caption{Schedule - Inputs}
\label{tab:sched}
\end{longtable}

\subsection{REQUIREMENTS}
Requirements are desired new features or enhancements to a software product.  It is important to know how many requirements
were scheduled to be completed versus how many actually got completed.  
\[
    S_5 = \frac{\sum^n_{i=1}\left( R_{a_i} - R_{e_i} \right)}{n}
\]
\begin{itemize}
\item $R_a$ actual requirements completed for application $i$
\item $R_e$ expected requirements completed for application $i$
\end{itemize}

\begin{longtable}{@{}l rr rr}
% pairs: absolute number (percentage)

\toprule%
 \centering%
 {\bfseries Column Name}
 & {\bfseries Data Type}
 &  \\

\cmidrule[0.4pt](r{0.125em}){1-1}%
\cmidrule[0.4pt](lr{0.125em}){2-2}%
\cmidrule[0.4pt](l{0.125em}){3-3}%
% \midrule
\endhead

Application ID & String  & Required \\
\myrowcolour%
Frequency Date & Date & Required \\
Requirements Scheduled & Integer & Required \\
\myrowcolour%
Actual Requirements Released & Integer  & Required \\

\bottomrule

\caption{Requirements - Inputs}
\label{tab:req}
\end{longtable}

\subsection{FINAL MPI SCORE}
One complete overall score cannot be successful without including information from various aspects of the software development organization.  Therefore, the previous results
are combined to provide one overall score.

\[
    MPI =\sum\limits^n_{i=1} w_i S_i \text{ where } \sum\limits^n_{i=1} w_i = 1
\]


\subsection{SENSITIVITY OF MPI}

For more on sensitivity analysis in statistical modeling, see \cite{Saltelli2000}.

\end{document}